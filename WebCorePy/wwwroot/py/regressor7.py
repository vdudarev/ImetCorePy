# -*- coding: utf-8 -*-
# запусть лучше в анаконде, в SQL Server - старый scikit-learn
# C:\ProgramData\Anaconda3\python.exe
# C:\Program Files\Microsoft SQL Server\MSSQL14.MSSQLSERVER\PYTHON_SERVICES\python.exe
# D:\WWW\IMET\IMETCorePy\2019_realValue4AspNetCore\DataFiles
# DOKUKIN params - begin
forest_params = dict(min_samples_leaf=3, n_estimators=50, random_state=6)
convex_combinations_params_loop = dict(n_combinations=400, generation_threshold=0.999, decorrelation_type='loop')
convex_combinations_params_final = dict(n_combinations=400, generation_threshold=0.999, decorrelation_type='final')
elnet_params=dict(normalize=True, max_iter=100000, l1_ratio=0.4)
# DOKUKIN params - end


ext = ""
csvSep = ";"
folderPath = "" #"D:\\WWW\\IMET\\IMETCorePy\\2019_realValue4AspNetCore\\DataFiles"
# Путь к файлам с исходными данными
sfilePathTrainExcel = ""
sfilePathPredictExcel = ""
sfilePathPredictExcelResults = ""
filePathTrainExcel = "" # folderPath + "\\TrainingSet.xls"    # обучающая выборка
filePathPredictExcel = "" # folderPath + "\\Prediction.xls"    # данные для прогнозирования
filePathPredictExcelResults = "" # folderPath + "\\Prediction_Results.xls"    # результаты прогнозирования
# Путь к файлу с логами
logFilePath = "" # folderPath + "\\Log.txt"


# Инициализация случайного значения (для воспроизводимости результатов) RND_init = 42
RND_init = 45
sSplit = None
X=[]
y=[]
json_object = None
frameLearn=None
framePredict=None
methodScoreData=[]
r2score = None
timeout4Method = 15     # 15 секунд на выполнение метода
language = "en"   # "ru" / "en"
cvFolds = 5    # 0 - Leave-One-Out (LOO) only; -N - LOO + N-fold; N - N-fold only



# Python code demonstrate how to create  
# Pandas DataFrame by lists of dicts. 
#import pandas as pd 
  
# Initialise data to lists. 
#data = [{'a': 1, 'b': 2, 'c':3}, {'a':10, 'b': 20, 'c': 30}] 
  
# Creates DataFrame. 
#df = pd.DataFrame(data) 
  
# Print the data 
#df 


# Uncomment this call when using matplotlib to generate images
# rather than displaying interactive UI.
#import matplotlib
#matplotlib.use('Agg')

import numbers
import numpy as np
import time
import sys
# [OPTIONAL] Seaborn makes plots nicer
import seaborn
import multiprocessing
import time
import json
import traceback

# вывод строки в файл и на экран
def write(input_text):
    print(input_text)
    # stdout is saved
    save_stdout = sys.stdout

    fh = open(logFilePath, "a") # "w" - перезаписывает, "a" - дописывает
    sys.stdout = fh
    print(input_text)

    # return to normal:
    sys.stdout = save_stdout
    fh.close()


# =====================================================================

# первый столбец содержит заголовок объекта, второй - целевая ф-ция (0 или 1), остальные - признаки
def read_data(filePathTrainExcel, filePathPredictExcel):
    global frameLearn, framePredict, sfilePathPredictExcel, csvSep
    '''
    Downloads the data for this script into a pandas DataFrame.
    '''
    if ext=="xls" or ext=="xlsx":
        from pandas import read_excel
        frameLearn = read_excel(
            filePathTrainExcel,
            header=0,       # заголовки в первой строке
            index_col=0,    # в первом столбце названия веществ (None если нет)
            # Uncomment if the file needs to be decompressed
        )
        if sfilePathPredictExcel!="":
            framePredict = read_excel(
                filePathPredictExcel,
                header=0,       # заголовки в первой строке
                index_col=0,    # в первом столбце названия веществ (None если нет)
                # Uncomment if the file needs to be decompressed
            )
    elif ext=="csv" or ext=="txt": 
        from pandas import read_csv
        frameLearn = read_csv(
            filePathTrainExcel,
            sep=csvSep,        # разделитель
            header=0,       # заголовки в первой строке
            index_col=0,    # в первом столбце названия веществ (None если нет)
            # Uncomment if the file needs to be decompressed
        )
        if sfilePathPredictExcel!="":
            framePredict = read_csv(
                filePathPredictExcel,
                sep=csvSep,     # разделитель
                header=0,       # заголовки в первой строке
                index_col=0,    # в первом столбце названия веществ (None если нет)
                # Uncomment if the file needs to be decompressed
            )
    else:
        raise NameError('Unsupported filetype: ' + ext)
    # Return the entire frames
    return (frameLearn, framePredict)


# =====================================================================

# получаем данные для обучения и тестирования
def get_features_and_labels(frameLearn, framePredict):
    global sfilePathPredictExcel, X, y, sSplit   # разрешение на МОДИФИКАЦИЮ глобальной переменной
    # '''
    # Transforms and scales the input data and returns numpy arrays for
    # training and testing inputs and targets.
    # '''
    # Replace missing values with 0.0, or we can use
    # scikit-learn to calculate missing values (below)
    #frame[frame.isnull()] = 0.0
    # Convert values to floats
    arr1 = np.array(frameLearn, dtype=float)
    if sfilePathPredictExcel!="":
        arr2 = np.array(framePredict, dtype=float)
    else:
        arr2 = None

    # Первый столбец - целевой, все остальные - X
    X, y = arr1[:, 1:], arr1[:, 0]
    X_train = X
    y_train = y
    if sfilePathPredictExcel!="":
        X_predict, y_predictEMPTY = arr2[:, 1:], arr2[:, 0]
    else:
        X_predict = None
        y_predictEMPTY = None
    # Normalize the attribute values to mean=0 and variance=1
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    # To scale to a specified range, use MinMaxScaler
    #from sklearn.preprocessing import MinMaxScaler
    #scaler = MinMaxScaler(feature_range=(0, 1))
    
    # Fit the scaler based on the training data, then apply the same
    # scaling to both training and test sets.
    scaler.fit(X_train)
    X_train = scaler.transform(X_train)
    if sfilePathPredictExcel!="":
        X_predict = scaler.transform(X_predict)
    # Return the training and test sets
    return X_train, X_predict, y_train, y_predictEMPTY



# =====================================================================
def DoCrossValidation(classifier, classifierName, sample_X, sample_y, CVscMode):
    global writer, cvFolds
    from sklearn.model_selection import cross_validate
    if cvFolds!=0:
        scores = cross_validate(classifier, sample_X, sample_y, cv=abs(cvFolds), scoring=CVscMode)        # scoring='f1_macro' - continuous is not supported        scoring='r2' (Dokukin)      'explained_variance'
    else:
        scores = None
    # 2022 - new BEGIN
    if cvFolds<=0:
        scoresLoo = cross_validate(classifier, sample_X, sample_y, cv=sample_X.shape[0], scoring=CVscMode)        # scoring='f1_macro' - continuous is not supported        scoring='r2' (Dokukin)      'explained_variance'
    else:
        scoresLoo = None
    # 2022 - new END
    #return scores
    return scores, scoresLoo


# ==============================
# вынесли это сюда, чтобы запускать эту опасность в другом потоке с контролем по времени
def evaluateRegressor_FitScore(classifier, X_train, y_train):
    # from sklearn.metrics import r2_score
    # reference_r2scs[j].append(r2_score(learn_y, reference_result))
    global r2score
    classifier.fit(X_train, y_train)
    r2score = classifier.score(X_train, y_train)


# =====================================================================
# X_learn, X_predict, y_learn, y_predict
def evaluateRegressor(classifier, classifierName, classifierMode, X_train, X_test, y_train, y_testEMPTY):
    global timeout4Method, frameLearn, framePredict, writer, methodScoreData, r2score, cvFolds
    '''
    Run multiple times with different classifiers to get an idea of the
    relative performance of each configuration.

    Returns a sequence of tuples containing:
        (title, precision, recall)
    for each learner.
    '''
    # We will calculate the P-R curve for each classifier
    from sklearn.metrics import precision_recall_curve, accuracy_score, roc_auc_score
    from sklearn.model_selection import cross_val_score
    from sklearn.metrics import r2_score
    # Test the linear support vector classifier
    start_time = time.time()    # засекли время начала
    # Fit the classifier
    repString = ""
    write("   " + time.strftime("%H:%M:%S", time.localtime()) + " Start evaluateRegressor(timeout=" + str(timeout4Method) + ") " + classifierName + "...")
    r2Loo, meanLoo, stdLoo = 0, 0, 0
    try:
        if timeout4Method<1:    # INPROCESS - Без защиты!
            evaluateRegressor_FitScore(classifier, X_train, y_train)
        else:
            p = multiprocessing.Process(target=evaluateRegressor_FitScore, args=(classifier, X_train, y_train))
            p.start()
            # Wait for 15 seconds or until process finishes
            p.join(timeout4Method)
            # If thread is still active
            if p.is_alive():
                write(classifierName + " is still running... let's kill it...")
                # Terminate - may not work if process is stuck for good
                p.terminate()
                # OR Kill - will work for sure, no chance for process to finish nicely however
                # p.kill()
                p.join()
                raise NameError(classifierName + " was killed, since took more than {0:.3f} seconds".format(timeout4Method))
            #else:
                #p.terminate()


        #evaluateRegressor_FitScore(classifier, X_train, y_train)
        #https://scikit-learn.org/stable/modules/model_evaluation.html
        CVscMode = 'explained_variance' #ARDRegression - лидер
        #CVscMode = 'r2'     # чуть хуже (ниже), чем 'explained_variance'
        #CVscMode = 'neg_mean_absolute_error'     # all is negative     -- НАДО
        #CVscMode = 'neg_mean_squared_error'     # all is negative       -- НАДО
        #CVscMode = 'neg_mean_squared_log_error'     # в половине алгоритмов ошибки при использовании 
        #CVscMode = 'neg_median_absolute_error'     # all is negative
        #scoresCV = CrossValidation(classifier, classifierName, X_train, y_train, CVscMode)
        scores, scoresLOO = DoCrossValidation(classifier, classifierName, X_train, y_train, ('neg_mean_absolute_error', 'neg_mean_squared_error', 'explained_variance'))
        scoresCV_Abs = None if scores is None else scores['test_neg_mean_absolute_error']
        scoresCV_Sq = None if scores is None else scores['test_neg_mean_squared_error']
        scoresCV_EV = None if scores is None else scores['test_explained_variance']
        scoresCV_AbsLoo = None if scoresLOO is None else scoresLOO['test_neg_mean_absolute_error']
        scoresCV_SqLoo = None if scoresLOO is None else scoresLOO['test_neg_mean_squared_error']
        scoresCV_EVLoo = None if scoresLOO is None else scoresLOO['test_explained_variance']
        # 2022 - BEGIN по аналогии от А.Докукина 
        # result, coef, r2Loo, meanLoo, stdLoo = leave_one_out_predict(X_train, y_train, classifier)
        if cvFolds<=0:
            result, r2Loo = leave_one_out_predict(X_train, y_train, classifier)
        else:
            result, r2Loo = None, None

        classifier.fit(X_train, y_train)    # обучение по всему!
        # 2022 - END - добавлено после LOO
        if sfilePathPredictExcel!="":
            y_predict = classifier.predict(X_test) # предсказания
            # framePredict[framePredict.columns[0] + ' [' + ("" if r2score>0.5 else "BAD_") + classifierName + "_{0:.3f}".format(r2score) + "__loo_{0:.3f}".format(r2Loo) + ']'] = y_predict
            loo_part = f"__loo_{r2Loo:.3f}" if r2Loo is not None else ""
            framePredict[framePredict.columns[0] + f" [{'' if r2score > 0.5 else 'BAD_'}{classifierName}_{r2score:.3f}{loo_part}]"] = y_predict

        #write("BEGIN ======== " + classifierName + " (R2={0:.3f})".format(r2score) + " ========")
        # write("len(X_train)={}".format(len(X_train)))
        # write("len(X_test)={}".format(len(X_test)))

        #for i in range(len(y_predict)): 
        #    write("Правильное значение: {0:.3f}; спрогнозированное значение: {1:.3f}. Дельта: {2:.3f}".format(y_test[i], y_predict[i], abs(y_test[i]-y_predict[i])))
    except BaseException as err:
        r2score = -1
        repString = "ERROR: " + classifierName + " " + str(err) + " " + traceback.format_exc()

    elapsed_time = time.time() - start_time    # получаем длительность
    #repString = classifierName+' (r2score={0:.3f}, time={1:.2f})'.format(r2score, elapsed_time) + " CrossValid("+CVscMode+"): Accuracy: %0.2f (+/- %0.2f)" % (scoresCV.mean(), scoresCV.std() * 2)
    if repString=="":
        #repString = classifierName + ' (r2score={0:.3f}, r2scoreLoo={1:.3f}, meanLoo={2:.3f}, stdLoo={3:.3f}, time={4:.2f})'.format(r2score, r2Loo, meanLoo, stdLoo, elapsed_time)
        loo_part = f", r2scoreLoo={r2Loo:.3f}" if r2Loo is not None else ""
        repString = classifierName + f" (r2score={r2score:.3f}{loo_part}, time={elapsed_time:.2f})"

        # repString = repString + " CrossValid(explained_variance): mean: %0.2f (std() * 2: +/- %0.2f)" % (scoresCV_EV.mean(), scoresCV_EV.std() * 2) + "; CrossValid(neg_mean_absolute_error): mean: %0.2f (std() * 2: +/- %0.2f)" % (abs(scoresCV_Abs.mean()), scoresCV_Abs.std() * 2) + "; CrossValid(neg_mean_squared_error): mean: %0.2f (std() * 2: +/- %0.2f)" % (abs(scoresCV_Sq.mean()), abs(scoresCV_Sq.std() * 2))
        if all(x is not None for x in (scoresCV_EV, scoresCV_Abs, scoresCV_Sq)):
            repString += (
                "; CrossValid(explained_variance): mean: %0.2f (std() * 2: +/- %0.2f)" % (scoresCV_EV.mean(), scoresCV_EV.std() * 2)
                + "; CrossValid(neg_mean_absolute_error): mean: %0.2f (std() * 2: +/- %0.2f)" % (abs(scoresCV_Abs.mean()), scoresCV_Abs.std() * 2)
                + "; CrossValid(neg_mean_squared_error): mean: %0.2f (std() * 2: +/- %0.2f)" % (abs(scoresCV_Sq.mean()), abs(scoresCV_Sq.std() * 2))
            )
        else:
            repString += " CrossValid(explained_variance): skipped (None)"

        # repString = repString + " LooCV(explained_variance): mean: %0.2f (std() * 2: +/- %0.2f)" % (scoresCV_EVLoo.mean(), scoresCV_EVLoo.std() * 2) + "; LooCV(neg_mean_absolute_error): mean: %0.2f (std() * 2: +/- %0.2f)" % (abs(scoresCV_AbsLoo.mean()), scoresCV_AbsLoo.std() * 2) + "; LooCV(neg_mean_squared_error): mean: %0.2f (std() * 2: +/- %0.2f)" % (abs(scoresCV_SqLoo.mean()), abs(scoresCV_SqLoo.std() * 2))
        if all(x is not None for x in (scoresCV_EVLoo, scoresCV_AbsLoo, scoresCV_SqLoo)):
            repString += (
                "; LooCV(explained_variance): mean: %0.2f (std() * 2: +/- %0.2f)" % (scoresCV_EVLoo.mean(), scoresCV_EVLoo.std() * 2)
                + "; LooCV(neg_mean_absolute_error): mean: %0.2f (std() * 2: +/- %0.2f)" % (abs(scoresCV_AbsLoo.mean()), scoresCV_AbsLoo.std() * 2)
                + "; LooCV(neg_mean_squared_error): mean: %0.2f (std() * 2: +/- %0.2f)" % (abs(scoresCV_SqLoo.mean()), abs(scoresCV_SqLoo.std() * 2))
            )
        else:
            repString += " LooCV(explained_variance): skipped (None)"

        # methodScoreData.append({'MethodName': classifierName, 'r2score': r2score, 'r2scoreLoo': r2Loo, 
        #     'CVExplainedVariance_mean': scoresCV_EV.mean(), 'CVExplainedVariance_std +/-': (scoresCV_EV.std() * 2),
        #     'CVNegMeanAbsoluteError_mean': abs(scoresCV_Abs.mean()), 'CVNegMeanAbsoluteError_std +/-': abs(scoresCV_Abs.std() * 2),
        #     'CVNegMeanSquaredError_mean': abs(scoresCV_Sq.mean()), 'CVNegMeanSquaredError_std +/-': abs(scoresCV_Sq.std() * 2),
        #     'LooCVExplainedVariance_mean': scoresCV_EVLoo.mean(), 'LooCVExplainedVariance_std +/-': (scoresCV_EVLoo.std() * 2),
        #     'LooCVNegMeanAbsoluteError_mean': abs(scoresCV_AbsLoo.mean()), 'LooCVNegMeanAbsoluteError_std +/-': abs(scoresCV_AbsLoo.std() * 2),
        #     'LooCVNegMeanSquaredError_mean': abs(scoresCV_SqLoo.mean()), 'LooCVNegMeanSquaredError_std +/-': abs(scoresCV_SqLoo.std() * 2),
        #     'time': elapsed_time, 'ErrorMessage': ''})
        get_val = lambda x, f: None if x is None else f(x)
        methodScoreData.append({
            'MethodName': classifierName,
            'r2score': r2score,
            'r2scoreLoo': r2Loo,
            'CVExplainedVariance_mean': get_val(scoresCV_EV, lambda v: v.mean()), 'CVExplainedVariance_std +/-': get_val(scoresCV_EV, lambda v: v.std() * 2),
            'CVNegMeanAbsoluteError_mean': get_val(scoresCV_Abs, lambda v: abs(v.mean())), 'CVNegMeanAbsoluteError_std +/-': get_val(scoresCV_Abs, lambda v: abs(v.std() * 2)),
            'CVNegMeanSquaredError_mean': get_val(scoresCV_Sq, lambda v: abs(v.mean())), 'CVNegMeanSquaredError_std +/-': get_val(scoresCV_Sq, lambda v: abs(v.std() * 2)),
            'LooCVExplainedVariance_mean': get_val(scoresCV_EVLoo, lambda v: v.mean()), 'LooCVExplainedVariance_std +/-': get_val(scoresCV_EVLoo, lambda v: v.std() * 2),
            'LooCVNegMeanAbsoluteError_mean': get_val(scoresCV_AbsLoo, lambda v: abs(v.mean())), 'LooCVNegMeanAbsoluteError_std +/-': get_val(scoresCV_AbsLoo, lambda v: abs(v.std() * 2)),
            'LooCVNegMeanSquaredError_mean': get_val(scoresCV_SqLoo, lambda v: abs(v.mean())), 'LooCVNegMeanSquaredError_std +/-': get_val(scoresCV_SqLoo, lambda v: abs(v.std() * 2)),
            'time': elapsed_time, 'ErrorMessage': ''
        })
    else:
        methodScoreData.append({'MethodName': classifierName, 'r2score': r2score, 'r2scoreLoo': r2Loo, 
            'CVExplainedVariance_mean': None, 'CVExplainedVariance_std +/-': None,
            'CVNegMeanAbsoluteError_mean': None, 'CVNegMeanAbsoluteError_std +/-': None,
            'CVNegMeanSquaredError_mean': None, 'CVNegMeanSquaredError_std +/-': None,
            'LooCVExplainedVariance_mean': None, 'LooCVExplainedVariance_std +/-': None,
            'LooCVNegMeanAbsoluteError_mean': None, 'LooCVNegMeanAbsoluteError_std +/-': None,
            'LooCVNegMeanSquaredError_mean': None, 'LooCVNegMeanSquaredError_std +/-': None,
            'time': elapsed_time, 'ErrorMessage': repString})

    write(repString)
    #write("END ======== " + classifierName + " ========")
    return repString, r2score, elapsed_time
# =====================================================================

def leave_one_out_predict(sample_X, sample_y, model):
    from sklearn.model_selection import LeaveOneOut
    from sklearn.metrics import r2_score
    loo = LeaveOneOut()
    result = np.zeros(sample_y.shape)
    #coef = np.zeros(sample_X.shape)    # LinearRegression only ?

    for train_index, test_index in loo.split(sample_X):
        # print(test_index)
        X_train, X_test = sample_X[train_index], sample_X[test_index]
        y_train, y_test = sample_y[train_index], sample_y[test_index]
        model.fit(X_train, y_train)
        result[test_index] = model.predict(X_test)
        #coef[test_index, :] = model.coef_      # LinearRegression only ?
    r2Loo = r2_score(sample_y, result)
    #meanLoo = np.mean(coef, axis=0)    # LinearRegression only ?
    #stdLoo = np.std(coef, axis=0)      # LinearRegression only ?
    return result, r2Loo    #, coef, meanLoo, stdLoo


# =====================================================================
# проверка классификатора
def procRegressor(classifier, classifierName, classifierMode, X_learn, X_predict, y_learn, y_predict, results):
    repString, scoreAcc, scoreAUC = evaluateRegressor(classifier, classifierName, classifierMode, X_train=X_learn, X_test=X_predict, y_train=y_learn, y_testEMPTY=y_predict)
    results.append((repString, scoreAcc, scoreAUC))


# проверка классификаторов из списка
def procAllRegressors(classifierList, X_learn, X_predict, y_learn, y_predict, results):
    for classifier, classifierName, classifierMode in classifierList:
        procRegressor(classifier, classifierName, classifierMode, X_learn=X_learn, X_predict=X_predict, y_learn=y_learn, y_predict=y_predict, results=results)

# =====================================================================


def readArgsJson():
    global json_object, timeout4Method, ext, folderPath, filePathTrainExcel, filePathPredictExcel, filePathPredictExcelResults, sfilePathTrainExcel, sfilePathPredictExcel, sfilePathPredictExcelResults, logFilePath, language, cvFolds
    #for idx, item in enumerate(sys.argv):
    #    write('Argument('+ str(idx) +') = ' + item)
    jsonFilePath = sys.argv[1]
    json_object = None
    with open(jsonFilePath, mode="r", encoding="utf-8") as content:
        json_object = json.load(content)

    folderPath = json_object["folderPath"] #sys.argv[1] #"D:\\MyProjects\\Python\\2019_realValue4AspNetCore\\DataFiles"
    # Путь к файлам с исходными данными
    sfilePathTrainExcel = json_object["fileTrain"] # sys.argv[2]
    sfilePathPredictExcel = json_object["filePredict"] # sys.argv[3] 
    timeout4Method = json_object["timeout4Method"] # 0
    language = json_object["language"] # "ru" / "en"
    cvFolds = json_object["cvFolds"] # 0 - Leave-One-Out (LOO) only; -N - LOO + N-fold; N - N-fold only
    #if len(sys.argv)>4:
    #    timeout4Method = int(sys.argv[4])

    import os.path
    ext = os.path.splitext(sfilePathTrainExcel)[1][1:].lower()

    #sfilePathPredictExcelResults = "result.xlsx"
    sfilePathPredictExcelResults = "result." + ext    # sys.argv[4]
    filePathTrainExcel = folderPath + "\\" + sfilePathTrainExcel    # folderPath + "\\ "TrainingSet.xls"    # обучающая выборка
    filePathPredictExcel = folderPath + "\\" + sfilePathPredictExcel # folderPath + "\\Prediction.xls"    # данные для прогнозирования
    filePathPredictExcelResults = folderPath + "\\" + sfilePathPredictExcelResults # folderPath + "\\Prediction_Results.xls"    # результаты прогнозирования
    # Путь к файлу с логами
    # logFilePath = folderPath + "\\" + sys.argv[5] # folderPath + "\\Log.txt"
    logFilePath = folderPath + "\\log.txt"

    # write("sys.version = " + sys.version + "; sys.executable = " + sys.executable + "; ext = '" + ext + "'")
    
    global filePathSANDMethodParam
    sfilePathMethodParam = "sandRegressor_134spinel.txt"
    filePathSANDMethodParam = folderPath + "\\py\\" + sfilePathMethodParam

    #write("sys.version = " + sys.version + "; sys.executable = " + sys.executable)
    #write('Argument List (' + str(len(sys.argv)) + ' items):' + str(sys.argv))

    #write("1. folderPath = {}".format(folderPath))
    #write("2. filePathTrainExcel = {}".format(filePathTrainExcel))
    #write("3. filePathPredictExcel = {}".format(filePathPredictExcel))
    #write("4. filePathPredictExcelResults = {}".format(filePathPredictExcelResults))
    #write("5. logFilePath = {}".format(logFilePath))
    #write("6. filePathSANDMethodParam = {}".format(filePathSANDMethodParam))


# https://scikit-learn.org/stable/supervised_learning.html#supervised-learning
def GetClassifierList(X_learn):
    global json_object
    # 1.1.1. Ordinary Least Squares
    # 1.1.2. Ridge regression and classification
    # 1.1.3. Lasso
    # - 1.1.4. Multi-task Lasso = MultiTaskLasso - # ValueError: For mono-task outputs, use ElasticNet
    # 1.1.5. Elastic-Net
    # - 1.1.6. Multi-task Elastic-Net = MultiTaskElasticNet - # ValueError: For mono-task outputs, use ElasticNet
    # + 1.1.7. Least Angle Regression   # куча варнингов и бред в результатах # classifierList.append((linear_model.Lars(), "linear_model.Lars", "predictONLY"))    # ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.662e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
    # 1.1.8. LARS Lasso
    # 1.1.9. Orthogonal Matching Pursuit (OMP)
    # 1.1.10.1. Bayesian Ridge Regression
    # 1.1.10.2. Automatic Relevance Determination - ARD
    # + 1.1.11. Logistic regression    # classifierList.append((linear_model.LogisticRegression(random_state=RND_init), "linear_model.LogisticRegression", "predict_proba")) # ValueError: Unknown label type: 'continuous'
    # + 1.1.12. Generalized Linear Regression - TweedieRegressor # на тест
    # + 1.1.13. Stochastic Gradient Descent - SGD - SGDRegressor # на тест # ValueError: Unknown label type:
    # + 1.1.14. Perceptron # на тест # ValueError: Unknown label type:
    # + 1.1.15. Passive Aggressive Algorithms # на тест 
    # + 1.1.16.2. RANSAC: RANdom SAmple Consensus# ValueError: min_samples may not be larger than number of samples X.shape[0].
    # 1.1.16.3. Theil-Sen estimator: generalized-median-based estimator
    # 1.1.16.4. Huber Regression
    # + 1.1.17. Quantile Regression
    from sklearn import linear_model
    from sklearn import discriminant_analysis
    # 1.3. Kernel ridge regression
    from sklearn.kernel_ridge import KernelRidge
    # + 1.4.2. Regression (1.4. Support Vector Machines): SVR, NuSVR and LinearSVR
    from sklearn import svm  #svm.SVR, svm.NuSVR, svm.LinearSVR
    # 1.5. Stochastic Gradient Descent => 1.1.13. Stochastic Gradient Descent
    # + 1.6.3. Nearest Neighbors Regression
    from sklearn import neighbors
    # + 1.7.1. Gaussian Process Regression (GPR)
    from sklearn.gaussian_process import GaussianProcessRegressor
    # + 1.8.3. PLSRegression
    from sklearn.cross_decomposition import PLSRegression
    # + 1.10. Decision Trees => 1.10.2. Regression
    from sklearn import tree
    # + 1.11.2.2. Extremely Randomized Trees
    # + 1.11.4. Gradient Tree Boosting
    # +++ПОДУМАТЬ 1.11.7. Voting Regressor
    from sklearn import ensemble
    # +++ПОДУМАТЬ 1.15. Isotonic regression
    from sklearn.cross_decomposition import PLSCanonical
    # 1.17.3. Neural network models (supervised) => Regression
    from sklearn import neural_network
    import sand3_09 as sand
    from convexforest import DecorrelatedConvexForestRegressor
    import fragment_r_03 as fragment
    import vpk_divergent as vpk_divergent

    # "predictONLY"   "predict_proba"   "decision_function"
    classifierList = []
    for algorithm in json_object["algorithms"]:
        classifier = None
        #print(f"   algorithm.name = {algorithm['name']}, algorithm.class = {algorithm['class']}")
        #print(f"classifier = {algorithm['class']}")
        #classifier = get_class("algorithm['class']")
        classifier = eval(algorithm['class'])
        #exec(f"classifier = {algorithm['class']}")
        #print(f"type(classifier) = {type(classifier).__name__}")
        for key, value in algorithm["settings"].items():
            #print(f" 0. {key} = {value}")
            #print('   typeof(value)=' + type(value).__name__)
            if type(value).__name__== 'str':
                exec(f'classifier.{key} = r"{value}"') # SyntaxError: EOL while scanning string literal
                #classifier.path = r'D:\MyProjects\Python\2019_realValue4AspNetCore\DataFiles\ '
                #exec(rf'classifier.{key} = """' + value + '"""') # добавление {}
                #exec(f'classifier.{key} = classifier.{key} + r"\"') # добавление {}
                #exec(f'classifier.%s = "%s"'%(key, value)) # добавление {}
                #exec(f'classifier.{key} = ''{value}''') # добавление {}
                #exec(f'classifier.{key} = list(''{value}'')[0]') # a bytes-like object is required, not 'str'
                #exec(f'classifier.{key} = \'{value}\'') # SyntaxError: EOL while scanning string literal
                #exec(r'classifier.{0} = "{1}"'.format(key, value)) # SyntaxError: EOL while scanning string literal
            else:
                exec(f'classifier.{key} = {value}')
            #print(f" 1. {key} = {value}")
        #print(f"classifier = {classifier}")
        classifierList.append((classifier, algorithm['name'], algorithm['mode']))
    #print("=== END LOOP ===")
    return classifierList


def procTask():
    global language, cvFolds
    readArgsJson()
    start_time = time.time()    # засекли время начала
    #for idx, item in enumerate(sys.argv):
    #    write('Argument('+ str(idx) +') = ' + item)

    write("BEGIN =================================================================")
    import sklearn as sk
    write('The scikit-learn version is {}.'.format(sk.__version__))
    write(f"0. language: {language}; timeout: {timeout4Method}; cvFolds[{cvFolds}]: {"no" if cvFolds==0 else str(abs(cvFolds))+"-fold"}; LOO: {"yes" if cvFolds<=0 else "no"}")
    write(("1. Читаем данные из файла {} для обучения." if language=="ru" else "1. Read data from the {} file for training.").format(sfilePathTrainExcel))
    if sfilePathPredictExcel!="":
        write(("1b. Читаем данные из файла {} для прогнозирования." if language=="ru" else "1b. Read data from the {} file for prediction.").format(sfilePathPredictExcel))

    frameLearn, framePredict = read_data(filePathTrainExcel, filePathPredictExcel)
    elapsed_time = time.time() - start_time    # получаем длительность
    if sfilePathPredictExcel!="":
        write(("2. Чтение выполнено за {0:.2f} сек. Обучаемся на объектах: {1} с {2} атрибутами. Прогнозируем объектов: {3}." if language=="ru" else "2. Reading completed in {0:.2f} seconds. Training on objects: {1} with {2} attributes. Predicting objects: {3}.").format(elapsed_time, len(frameLearn.index), len(frameLearn.columns), len(framePredict.index)))
    else:
        framePredict=None
        write(("2. Чтение выполнено за {0:.2f} сек. Обучаемся на объектах: {1} с {2} атрибутами." if language=="ru" else "2. Reading completed in {0:.2f} seconds. Training on objects: {1} with {2} attributes.").format(elapsed_time, len(frameLearn.index), len(frameLearn.columns)))

    # Process data into feature and label arrays
    start_time = time.time()    # засекли время начала
    X_learn, X_predict, y_learn, y_predict = get_features_and_labels(frameLearn, framePredict)
    elapsed_time = time.time() - start_time    # получаем длительность
    if sfilePathPredictExcel!="":
        write(("3. Отмасштабированы признаки и подготовлены выборки (обучение+прогнозирование) за {:.2f} сек." if language=="ru" else "3. Features scaled and samples prepared (training + prediction) in {:.2f} seconds.").format(elapsed_time))
    else:
        write(("3. Отмасштабированы признаки и подготовлена выборка для обучения за {:.2f} сек." if language=="ru" else "3. Features scaled and training sample prepared in {:.2f} seconds.").format(elapsed_time))
    # write("Простые методы:")
    results = []

    classifierList = GetClassifierList(X_learn)

    procAllRegressors(classifierList=classifierList, X_learn=X_learn, X_predict=X_predict, y_learn=y_learn, y_predict=y_predict, results=results)
    
    # сохраним результат оценки классификаторов
    import pandas as pd 
    dfScore = pd.DataFrame(methodScoreData, columns = ['MethodName', 'r2score', 'r2scoreLoo', 
'CVExplainedVariance_mean', 'CVExplainedVariance_std +/-', 
'CVNegMeanAbsoluteError_mean', 'CVNegMeanAbsoluteError_std +/-', 
'CVNegMeanSquaredError_mean', 'CVNegMeanSquaredError_std +/-', 
'LooCVExplainedVariance_mean', 'LooCVExplainedVariance_std +/-', 
'LooCVNegMeanAbsoluteError_mean', 'LooCVNegMeanAbsoluteError_std +/-', 
'LooCVNegMeanSquaredError_mean', 'LooCVNegMeanSquaredError_std +/-', 
'time', 'ErrorMessage']) 
    if ext=="xls" or ext=="xlsx":
        save_excel(dfScore, folderPath + "\\resultScore." + ext, ext)
        # from pandas import ExcelWriter
        # writer = ExcelWriter(folderPath + "\\resultScore." + ext)
        # dfScore.to_excel(writer, 'Results')
        # writer.save()
        write(("4. Файл с результатами оценки алгоритмов сохранен: " if language=="ru" else "4. The file with the algorithm evaluation results saved: ") + "resultScore." + ext)
    elif ext=="csv" or ext=="txt": 
        # framePredict.to_csv(filePathPredictExcelResults, sep=csvSep, encoding='utf-8')    # without BOM
        dfScore.to_csv(folderPath + "\\resultScore." + ext, sep=csvSep, encoding='utf-8-sig')  # with BOM
        write(("4. Файл с результатами оценки алгоритмов сохранен: " if language=="ru" else "4. The file with the algorithm evaluation results saved: ") + "resultScore." + ext)
    else:
        raise NameError('Unsupported filetype: ' + ext)



    # сохраним результат прогнозирования
    if sfilePathPredictExcel!="":
        if ext=="xls" or ext=="xlsx":
            save_excel(framePredict, filePathPredictExcelResults, ext)
            # from pandas import ExcelWriter
            # writer = ExcelWriter(filePathPredictExcelResults)
            # framePredict.to_excel(writer, 'Results')
            # writer.save()
            # f"__loo_{r2Loo:.3f}" if r2Loo is not None else ""
            write(("5. Файл с результатами сохранен: " if language=="ru" else "5. File with results was saved: ") + sfilePathPredictExcelResults)
        elif ext=="csv" or ext=="txt": 
            # framePredict.to_csv(filePathPredictExcelResults, sep=csvSep, encoding='utf-8')    # without BOM
            framePredict.to_csv(filePathPredictExcelResults, sep=csvSep, encoding='utf-8-sig')  # with BOM
            write(("5. Файл с результатами сохранен: " if language=="ru" else "5. File with results was saved: ") + sfilePathPredictExcelResults)
        else:
            raise NameError('Unsupported filetype: ' + ext)
    
    write("END =================================================================")


# universal save of DataFrame to Excel (.xls or .xlsx),
def save_excel(df: pd.DataFrame, path: str, ext: str):
    if ext == "xlsx":
        try:
            df.to_excel(path, index=False, engine="openpyxl")
        except ImportError:
            raise ImportError("To write .xlsx we need openpyxl package. Install it: pip install openpyxl")
    elif ext == "xls":
        try:
            import xlwt
            wb = xlwt.Workbook()
            ws = wb.add_sheet("Sheet1")

            # Заголовки
            for j, col in enumerate(df.columns):
                ws.write(0, j, col)

            # Данные
            for i, row in enumerate(df.itertuples(index=False), start=1):
                for j, value in enumerate(row):
                    ws.write(i, j, str(value) if value is not None else "")
            
            wb.save(path)

        except ImportError:
            raise ImportError("To write .xls we need xlwt package. Install it: pip install xlwt")

    else:
        raise ValueError(f"Unsupported file extension: {ext}")


if __name__ == '__main__':
    procTask()
    quit()
